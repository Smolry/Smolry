I have created a simple neural network using mnist data set from scratch just to understand it better

1) Building blocks: Neurons
- take in 2 inputs,does some computation adn gives output.

  inputs:
  ![alt text](image.png)
- 3 steps for that:~

1. each input is multipied by weights:~

          x1â†’x1âˆ—w1
          ğ‘¥2â†’ğ‘¥2âˆ—ğ‘¤2

2.all the weights are added together bias b:~
          
          
          (x1âˆ—w1)+(x2âˆ—w2)+b
*(Simply put, bias is a constant term addded to the function to offset the result.Result can be seen when activation function is used, to shift the model toward +ve or -ve.)
 
3.Finally,the sum is passed through an activation function:
          
          y=f(x1*w1+x2*w2+b)
The activation function is used to turn an unbounded input into an output that has a nice, predictable form. A commonly used activation function is the sigmoid function:

sigmoid

The sigmoid function only outputs numbers in the range 
(
0
,
1
)
(0,1). You can think of it as compressing 
(
âˆ’
âˆ
,
+
âˆ
)
(âˆ’âˆ,+âˆ) to 
(
0
,
1
)
(0,1) - big negative numbers become ~
0
0, and big positive numbers become ~
1
1.

2)Combining neurons: 

a neural network is nothing more than a bunch of neurons connected together.

![
](image-1.png)

This network has 2 inputs, a hidden layer with 2 neurons (
â„
1
h 
1
â€‹
  and 
â„
2
h 
2
â€‹
 ), and an output layer with 1 neuron (
ğ‘œ
1
o 
1
â€‹
 ). Notice that the inputs for 
ğ‘œ
1
o 
1
â€‹
  are the outputs from 
â„
1
h 
1
â€‹
  and 
â„
2
h 
2
â€‹
  - thatâ€™s what makes this a network.

A hidden layer is any layer between the input (first) layer and output (last) layer. There can be multiple hidden layers!


An Example: Feedforward
Letâ€™s use the network pictured above and assume all neurons have the same weights ğ‘¤=[0,1]w=[0,1], the same bias ğ‘=0
b=0, and the same sigmoid activation function. Let 
â„1,â„2,ğ‘œ1h1,h2â€‹,o1
â€‹
  denote the outputs of the neurons they represent.

What happens if we pass in the input 
ğ‘¥=[2,3]
x=[2,3]?

â„1=â„2=ğ‘“(ğ‘¤â‹…ğ‘¥+ğ‘)=ğ‘“((0âˆ—2)+(1âˆ—3)+0)=ğ‘“(3)=0.9526
h1=h2
â€‹
 
â€‹
  
=f(wâ‹…x+b)
=f((0âˆ—2)+(1âˆ—3)+0)
=f(3)
=0.9526
â€‹
 
ğ‘œ1=ğ‘“(ğ‘¤â‹…[â„1,â„2]+ğ‘)=ğ‘“((0âˆ—h1)+(1âˆ—â„2)+0)=f(0.9526)=0.7216
o1=f(wâ‹…[h1,h2]+b)=f((0âˆ—h1)+(1âˆ—h2)+0)=f(0.9526)= 0.7216
â€‹
 
â€‹
 
The output of the neural network for input 
ğ‘¥=[2,3]
x=[2,3] is 
0.7216
0.7216

3. Training a Neural Network, Part 1
Say we have the following measurements:

Name	Weight (kg)	Height (in)	Gender
Ani	80	70	M
Blud	71	65	F
orry	64	60	M
sanz	74	72	F




Weâ€™ll represent Male with a 
0
 and Female with a 
1, and weâ€™ll also shift the data to make it easier to use:

Name	Weight (minus 61.29)	Height (minus 66)	Gender
Ani	  18.71	4	0

Blud	9.71	-1	1

orry	2.71	-6	0

Diana	12.71	6	1


I arbitrarily chose the shift amounts (
135
135 and 
66
66) to make the numbers look nice. Normally, youâ€™d shift by the mean.

Loss
Before we train our network, we first need a way to quantify how â€œgoodâ€ itâ€™s doing so that it can try to do â€œbetterâ€. Thatâ€™s what the loss is.

Weâ€™ll use the mean squared error (MSE) loss:MSE=1ğ‘›âˆ‘ğ‘–=1ğ‘›(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’âˆ’ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘)^2

MSE= n1i=1âˆ‘nâ€‹(ytrueâˆ’ypred)^2
 
Letâ€™s break this down:

ğ‘›

n is the number of samples, which is 
4

ğ‘¦

y represents the variable being predicted, which is Gender.

ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’

ytrue is the true value of the variable (the â€œcorrect answerâ€). For example, 

ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘

ypred is the predicted value of the variable. Itâ€™s whatever our network outputs.
(
ğ‘¦
ğ‘¡
ğ‘Ÿ
ğ‘¢
ğ‘’
âˆ’
ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘
)
2
(y 
true
â€‹
 âˆ’y 
pred
â€‹
 ) 
2
  is known as the squared error. Our loss function is simply taking the average over all squared errors (hence the name mean squared error). The better our predictions are, the lower our loss will be!

Better predictions = Lower loss.

Training a network = trying to minimize its loss.

An Example Loss Calculation
Letâ€™s say our network always outputs 
0
0 - in other words, itâ€™s confident all humans are Male ğŸ¤”. What would our loss be?

Name	
ğ‘¦
ğ‘¡
ğ‘Ÿ
ğ‘¢
ğ‘’
y 
true
â€‹
 	
ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘
y 
pred
â€‹
 	
(
ğ‘¦
ğ‘¡
ğ‘Ÿ
ğ‘¢
ğ‘’
âˆ’
ğ‘¦
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘‘
)
2
(y 
true
â€‹
 âˆ’y 
pred
â€‹
 ) 
2
 
Ani	0	0	0

Blud	1	0	1

orry	0	0	0

sanz	1	0	1

MSE=14(1+0+0+1)=
0.5

MSE= 41(1+0+0+1)= 
0.5


â€‹4. Training a Neural Network, Part 2
We now have a clear goal:

 minimize the loss of the neural network. We know we can change the networkâ€™s weights and biases to influence its predictions, but how do we do so in a way that decreases loss?

This section uses a bit of multivariable calculus. If youâ€™re not comfortable with calculus, feel free to skip over the math parts.

For simplicity, letâ€™s pretend we only have Ani in our dataset:

Name	Weight (minus 135)	Height (minus 66)	Gender
Ani	18.71	4	0
Then the mean squared error loss is just Aniâ€™s squared error:

MSE=11âˆ‘ğ‘–=11(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’âˆ’ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘)2=(ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’âˆ’ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘)2=(1âˆ’ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘)2

MSE= 11â€‹i=1âˆ‘1(ytrueâˆ’y pred) 2=(y trueâˆ’y pred) 2=(1âˆ’y pred) 2
 
â€‹
 
Another way to think about loss is as a function of weights and biases. Letâ€™s label each weight and bias in our network:
![alt text](image-2.png)



Then, we can write loss as a multivariable function:

![
](image-3.png)

![alt text](image-4.png)

![alt text](image-5.png)
Training: Stochastic Gradient Descent
We have all the tools we need to train a neural network now! Weâ€™ll use an optimization algorithm called stochastic gradient descent (SGD) that tells us how to change our weights and biases to minimize loss. Itâ€™s basically just this update equation:

ğ‘¤
1
â†
ğ‘¤
1
âˆ’
ğœ‚
âˆ‚
ğ¿
âˆ‚
â€‹
 
ğœ‚
Î· is a constant called the learning rate that controls how fast we train. All weâ€™re doing is subtracting 
ğœ‚
âˆ‚
ğ¿
âˆ‚
ğ‘¤
1
Î· 
âˆ‚w 
1
â€‹
 
âˆ‚L
â€‹

  from 
ğ‘¤
1
w 
1
â€‹
 :

If 
âˆ‚
ğ¿
âˆ‚
ğ‘¤
1
âˆ‚w 
1
â€‹
 
âˆ‚L
â€‹

  is positive, 
ğ‘¤
1
w 
1
â€‹
  will decrease, which makes 
ğ¿
L decrease.
If 
âˆ‚
ğ¿
âˆ‚
ğ‘¤
1
âˆ‚w 
1
â€‹
 
âˆ‚L
â€‹
  is negative, 
ğ‘¤
1
w 
1
â€‹
  will increase, which makes 
ğ¿
L decrease.
If we do this for every weight and bias in the network, the loss will slowly decrease and our network will improve.

Our training process will look like this:

Choose one sample from our dataset. This is what makes it stochastic gradient descent - we only operate on one sample at a time.
Calculate all the partial derivatives of loss with respect to weights or biases (e.g. 
âˆ‚
ğ¿
âˆ‚
ğ‘¤
1
âˆ‚w 
1
â€‹
 
âˆ‚L
â€‹
 , 
âˆ‚
ğ¿
âˆ‚
ğ‘¤
2
âˆ‚w 
2
â€‹
 
âˆ‚L
â€‹
 , etc).
Use the update equation to update each weight and bias.
Go back to step 1.



